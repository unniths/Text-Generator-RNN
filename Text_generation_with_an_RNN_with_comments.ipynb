{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Text generation with an RNN with comments.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unniths/Text-Generator-RNN/blob/master/Text_generation_with_an_RNN_with_comments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-NJ8L8mdLQ1",
        "colab_type": "text"
      },
      "source": [
        "## Shiva Unnithan\n",
        "# Text generation with an RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEWbvfc3dLQ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48BsfnnIdLQ-",
        "colab_type": "code",
        "outputId": "bbe7b96e-15b5-4f52-99e2-695c590d4824",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bK3tF6LAdLRC",
        "colab_type": "code",
        "outputId": "35f14dcc-ee95-43d4-c0cd-6412bbcd94ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Read, then decode for py2 compat\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} character'.format(len(text)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 character\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTWByUlldLRI",
        "colab_type": "code",
        "outputId": "00803b0b-3022-4aa4-c178-8be30c8bc0f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "print(text[:1000]) # printing the first 1000 characters"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lfYxRN4dLRN",
        "colab_type": "code",
        "outputId": "c395927f-2e42-473e-90d0-5df23c8a8047",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print ('{} unique character'.format(len(vocab)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65 unique character\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5HbURAIdLRS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a mapping from unique characters to indices \n",
        "char2idx = {u:i for i, u in enumerate(vocab)} # look up table to map characters to numbers\n",
        "idx2char = np.array(vocab) \n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text]) # make each character an integer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gh42c8WdLRW",
        "colab_type": "code",
        "outputId": "aabe03f8-d548-4412-92c0-923cb099ae50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)): # printing the range 0 to 20 showing just a piece of every character turned into an int\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char])) #formatting to see which character represents which integer\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  '!' :   2,\n",
            "  '$' :   3,\n",
            "  '&' :   4,\n",
            "  \"'\" :   5,\n",
            "  ',' :   6,\n",
            "  '-' :   7,\n",
            "  '.' :   8,\n",
            "  '3' :   9,\n",
            "  ':' :  10,\n",
            "  ';' :  11,\n",
            "  '?' :  12,\n",
            "  'A' :  13,\n",
            "  'B' :  14,\n",
            "  'C' :  15,\n",
            "  'D' :  16,\n",
            "  'E' :  17,\n",
            "  'F' :  18,\n",
            "  'G' :  19,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HNui02GdLRb",
        "colab_type": "code",
        "outputId": "6c07baa9-c8b9-4fa9-8e3f-438a5a1f9cf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('{} ---- characters mapped to int ----> {}'.format(repr(text[:13]), text_as_int[:13])) # Quick example of how First Citizen is mapped in integers using the method"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen' ---- characters mapped to int ----> [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MSgMc26dLRg",
        "colab_type": "code",
        "outputId": "b496e25b-ea54-426f-8a26-9254e0e0e67e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 1000 # length of text measured by characters\n",
        "examples_per_epoch = len(text)//(seq_length+1) \n",
        "\n",
        "# Create training examples / target\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int) # Converting the text vector into a steam of character indices\n",
        "\n",
        "for i in char_dataset.take(5): #take the first 5 characters from the dataset\n",
        "    print(idx2char[i.numpy()]) #print the first five indices from the array using idx2char"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mM1eWkbcdLRk",
        "colab_type": "code",
        "outputId": "e9a81000-4b4a-45e8-eca8-11d49aa43516",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True) # in dataset, batch converts individual characters to sequences or chunks\n",
        "\n",
        "for item in sequences.take(5): # 5 is the desired length for the sequence\n",
        "    print(repr(''.join(idx2char[item.numpy()]))) "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to particularise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we become rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\nS\"\n",
            "\"econd Citizen:\\nWould you proceed especially against Caius Marcius?\\n\\nAll:\\nAgainst him first: he's a very dog to the commonalty.\\n\\nSecond Citizen:\\nConsider you what services he has done for his country?\\n\\nFirst Citizen:\\nVery well; and could be content to give him good\\nreport fort, but that he pays himself with being proud.\\n\\nSecond Citizen:\\nNay, but speak not maliciously.\\n\\nFirst Citizen:\\nI say unto you, what he hath done famously, he did\\nit to that end: though soft-conscienced men can be\\ncontent to say it was for his country he did it to\\nplease his mother and to be partly proud; which he\\nis, even till the altitude of his virtue.\\n\\nSecond Citizen:\\nWhat he cannot help in his nature, you account a\\nvice in him. You must in no way say he is covetous.\\n\\nFirst Citizen:\\nIf I must not, I need not be barren of accusations;\\nhe hath faults, with surplus, to tire in repetition.\\nWhat shouts are these? The other side o' the city\\nis risen: why stay we prating here? to the Capitol!\\n\\nAll:\\nCome, come.\\n\\nFirst Cit\"\n",
            "\"izen:\\nSoft! who comes here?\\n\\nSecond Citizen:\\nWorthy Menenius Agrippa; one that hath always loved\\nthe people.\\n\\nFirst Citizen:\\nHe's one honest enough: would all the rest were so!\\n\\nMENENIUS:\\nWhat work's, my countrymen, in hand? where go you\\nWith bats and clubs? The matter? speak, I pray you.\\n\\nFirst Citizen:\\nOur business is not unknown to the senate; they have\\nhad inkling this fortnight what we intend to do,\\nwhich now we'll show 'em in deeds. They say poor\\nsuitors have strong breaths: they shall know we\\nhave strong arms too.\\n\\nMENENIUS:\\nWhy, masters, my good friends, mine honest neighbours,\\nWill you undo yourselves?\\n\\nFirst Citizen:\\nWe cannot, sir, we are undone already.\\n\\nMENENIUS:\\nI tell you, friends, most charitable care\\nHave the patricians of you. For your wants,\\nYour suffering in this dearth, you may as well\\nStrike at the heaven with your staves as lift them\\nAgainst the Roman state, whose course will on\\nThe way it takes, cracking ten thousand curbs\\nOf more strong link asunder than can eve\"\n",
            "\"r\\nAppear in your impediment. For the dearth,\\nThe gods, not the patricians, make it, and\\nYour knees to them, not arms, must help. Alack,\\nYou are transported by calamity\\nThither where more attends you, and you slander\\nThe helms o' the state, who care for you like fathers,\\nWhen you curse them as enemies.\\n\\nFirst Citizen:\\nCare for us! True, indeed! They ne'er cared for us\\nyet: suffer us to famish, and their store-houses\\ncrammed with grain; make edicts for usury, to\\nsupport usurers; repeal daily any wholesome act\\nestablished against the rich, and provide more\\npiercing statutes daily, to chain up and restrain\\nthe poor. If the wars eat us not up, they will; and\\nthere's all the love they bear us.\\n\\nMENENIUS:\\nEither you must\\nConfess yourselves wondrous malicious,\\nOr be accused of folly. I shall tell you\\nA pretty tale: it may be you have heard it;\\nBut, since it serves my purpose, I will venture\\nTo stale 't a little more.\\n\\nFirst Citizen:\\nWell, I'll hear it, sir: yet you must not think to\\nfob off our\"\n",
            "\" disgrace with a tale: but, an 't please\\nyou, deliver.\\n\\nMENENIUS:\\nThere was a time when all the body's members\\nRebell'd against the belly, thus accused it:\\nThat only like a gulf it did remain\\nI' the midst o' the body, idle and unactive,\\nStill cupboarding the viand, never bearing\\nLike labour with the rest, where the other instruments\\nDid see and hear, devise, instruct, walk, feel,\\nAnd, mutually participate, did minister\\nUnto the appetite and affection common\\nOf the whole body. The belly answer'd--\\n\\nFirst Citizen:\\nWell, sir, what answer made the belly?\\n\\nMENENIUS:\\nSir, I shall tell you. With a kind of smile,\\nWhich ne'er came from the lungs, but even thus--\\nFor, look you, I may make the belly smile\\nAs well as speak--it tauntingly replied\\nTo the discontented members, the mutinous parts\\nThat envied his receipt; even so most fitly\\nAs you malign our senators for that\\nThey are not such as you.\\n\\nFirst Citizen:\\nYour belly's answer? What!\\nThe kingly-crowned head, the vigilant eye,\\nThe counsellor he\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4Y6T2F_dLRp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk): \n",
        "    input_text = chunk[:-1] # seq_length -> input\n",
        "    target_text = chunk[1:] # seq_length+1 -> target\n",
        "    return input_text, target_text # the values of -1 and 1 have to do with the tanh squashing function which is used in RNN.\n",
        "\n",
        "dataset = sequences.map(split_input_target) # using map function to apply this method to each batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvVV0SUUdLRx",
        "colab_type": "code",
        "outputId": "1857bbd2-ff90-4db7-9572-1624ff9a8411",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# This prints the example input value and target value, which also shows the way the data works\n",
        "for input_example, target_example in dataset.take(1): \n",
        "    print('Input data: ', repr(''. join(idx2char[input_example.numpy()])))\n",
        "    print('Target data: ', repr(''.join(idx2char[target_example.numpy()]))) "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  \"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to particularise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we become rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\n\"\n",
            "Target data:  \"irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to particularise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we become rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\nS\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nnv31rcvdLR1",
        "colab_type": "code",
        "outputId": "2a450128-ede0-484b-9b5f-71df05243c21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "# This part is showing how the model is predicting the next character. So the model first gets the index for F, and expects 'i'. When it puts the input of i, RNN means it would remember the previous results and continue with preds.\n",
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:10], target_example[:10])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "  input: 18 ('F')\n",
            "  expected output: 47 ('i')\n",
            "Step    1\n",
            "  input: 47 ('i')\n",
            "  expected output: 56 ('r')\n",
            "Step    2\n",
            "  input: 56 ('r')\n",
            "  expected output: 57 ('s')\n",
            "Step    3\n",
            "  input: 57 ('s')\n",
            "  expected output: 58 ('t')\n",
            "Step    4\n",
            "  input: 58 ('t')\n",
            "  expected output: 1 (' ')\n",
            "Step    5\n",
            "  input: 1 (' ')\n",
            "  expected output: 15 ('C')\n",
            "Step    6\n",
            "  input: 15 ('C')\n",
            "  expected output: 47 ('i')\n",
            "Step    7\n",
            "  input: 47 ('i')\n",
            "  expected output: 58 ('t')\n",
            "Step    8\n",
            "  input: 58 ('t')\n",
            "  expected output: 47 ('i')\n",
            "Step    9\n",
            "  input: 47 ('i')\n",
            "  expected output: 64 ('z')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roVVb_c7dLR4",
        "colab_type": "text"
      },
      "source": [
        "## Create Training Batches\n",
        "https://www.tensorflow.org/tutorials/text/text_generation#create_training_batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkfAGdxudLR5",
        "colab_type": "code",
        "outputId": "abc9ad2b-4225-4efb-ea90-84c4dccb2371",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Batch size means how big a batch of data that was split from the text source will be\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset \n",
        "# (TF data is design to work with possibly infinite sequences, so it doesn't attempt to shuffle the entire sequence in memory.\n",
        "# Instead, it maintains a buffer in which is shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 1000), (64, 1000)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MI5wC8KxdLR_",
        "colab_type": "text"
      },
      "source": [
        "## Build The Model\n",
        "https://www.tensorflow.org/tutorials/text/text_generation#build_the_model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j11YxvU7dLSA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a789bdab-f4d5-4d91-92fd-67a257452165"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension \n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units \n",
        "rnn_units = 1024\n",
        "print(vocab_size)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "au-4X8KUdLSF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([ # Sequential is used to group a linear stack of layers. Used since all the layers have a single input and produce a single output. \n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]), # Embedding is the input layer. A trainable lookup table that will map numbers of each char to a vector using embedding_dim dimensions.\n",
        "        tf.keras.layers.GRU(rnn_units, # SWITCHING TO LSTM LAYER INSTEAD, GRU IS A TYPE OF RNN WITH A SIZE TO MAKE IT MORE ACCURATE\n",
        "                           return_sequences=True,\n",
        "                           stateful=True,\n",
        "                           recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dense(vocab_size) # Output layer, output are determined by vocab_size\n",
        "    ])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Snez7aifdLSJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "    vocab_size = len(vocab), # 65\n",
        "    embedding_dim=embedding_dim, #using embedding dm which we declared as 256\n",
        "    rnn_units=rnn_units, # using rnn_unnits which was declared at 1024 \n",
        "    batch_size=BATCH_SIZE) # using BATCH_SIZE which was declared as 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLwsr2SZdLSM",
        "colab_type": "text"
      },
      "source": [
        "## Try the Model\n",
        "https://www.tensorflow.org/tutorials/text/text_generation#try_the_model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vc1I3qVXdLSN",
        "colab_type": "code",
        "outputId": "9e2d931c-cc3f-4d20-e026-cf7d8679d302",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Testing the model\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 1000, 65) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZkbuU3PdLSR",
        "colab_type": "code",
        "outputId": "f7fc789e-96e7-4bfb-9de4-a0e75ed64f5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model.summary() # summary to show every individual layer of the model"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYZ_LZ19dLSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1) # First examle in the batch\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy() # removing a specific axis of size (-1) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKMptTa6dLSc",
        "colab_type": "code",
        "outputId": "c2c8c8cc-5aa0-4f3b-a6c6-88c96362935a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([22, 60,  0, 49, 54, 50, 48, 61, 23, 49, 25, 13,  9, 43, 64, 21, 26,\n",
              "       23, 46, 51, 21, 42, 20, 34, 55,  5, 50, 47, 37, 64,  7, 49, 16, 24,\n",
              "       64, 44, 62,  8, 57, 47,  3, 14, 54, 22, 61, 22, 41, 47, 35, 47, 35,\n",
              "       32,  9,  8, 45,  3, 19, 19, 25, 25, 43, 23, 29, 59, 44, 40,  6, 18,\n",
              "       18, 22, 10, 58, 39,  5, 31, 10, 32, 47, 41, 57, 54, 61, 52,  4, 41,\n",
              "       57,  8, 17, 62,  9, 38,  6, 51, 40,  4, 54,  1, 50, 14, 42,  4, 42,\n",
              "       42, 64, 19, 32,  6, 54, 35, 57, 45, 23,  9, 16, 35, 21, 24,  7, 43,\n",
              "       64, 43, 34, 32, 21,  2, 62, 30, 49, 49, 17, 12,  7,  1, 11, 37, 33,\n",
              "       57, 61, 30, 15, 36, 37, 42, 37, 30, 62,  0,  6, 26, 62, 38, 42, 12,\n",
              "        6, 12, 15,  2, 34, 29, 38, 41, 18,  0, 62, 33,  3, 31, 56, 11,  8,\n",
              "       39, 51, 28, 39, 42, 32, 56,  3, 64, 49, 18, 23,  0, 22,  6, 11, 30,\n",
              "       41, 50, 31, 30, 21,  3, 18,  5, 36, 11, 62, 39, 29,  5, 38, 10,  2,\n",
              "        1, 49, 21, 12,  1,  7, 57, 52,  9, 61,  4, 44,  2, 43, 56, 31, 54,\n",
              "       16, 56,  6, 18, 48, 47, 14, 42, 62, 45, 57, 31, 48, 49, 36,  6, 50,\n",
              "       48, 46, 37, 21, 36, 13, 59,  5, 26,  7,  9, 13, 57, 50, 52, 53, 54,\n",
              "       30, 62,  6, 39, 50, 12, 52, 61, 32, 26, 60, 16, 25, 62, 36, 29,  5,\n",
              "        4, 60, 48, 44, 40, 41,  2, 48, 16, 53, 20, 47, 20,  0, 45, 31, 34,\n",
              "        2, 40, 50, 22, 25, 64, 58,  5,  8, 60, 19, 60, 40, 60, 29, 60, 25,\n",
              "       41, 64,  2, 30, 32, 15, 47, 35, 49, 54, 22, 41, 29, 12,  7, 54, 57,\n",
              "       31, 26, 22, 13, 52, 45, 41, 63,  7, 46, 16, 33, 11, 56,  5, 56, 45,\n",
              "       34, 26, 58,  7, 19,  8,  8, 23, 17, 63,  6, 57, 55, 19, 11, 63,  5,\n",
              "       44, 23, 45, 43, 57,  3, 32, 44, 30, 49, 33, 37, 21, 23, 47,  3, 10,\n",
              "       22,  2, 25, 18,  5, 45, 39, 63, 17, 14,  2, 19, 47, 57, 21, 40, 19,\n",
              "       42, 47, 34,  3, 62,  1, 21, 64,  0, 58, 63,  8, 25,  9, 52,  6, 48,\n",
              "       50, 41, 51, 45, 62, 34, 48, 48,  4,  9, 38, 24,  0,  3, 28,  9, 51,\n",
              "        3, 26, 62,  6, 49, 47, 19, 28, 54, 22,  7, 17, 19,  5, 40, 56, 21,\n",
              "       32, 12, 35, 11, 29, 36,  2, 35, 54, 16, 56,  0, 58, 31,  5, 48,  8,\n",
              "       31, 57, 23, 17,  0,  4, 53, 13,  6, 48, 59, 30,  0,  3, 10, 60,  0,\n",
              "       28, 55, 41,  0, 32, 29, 49, 14, 35, 20, 54, 48, 45, 34, 38,  1, 36,\n",
              "       64, 63, 48,  3, 18, 39, 59, 22, 61, 20, 21, 60, 44, 54, 47, 33, 54,\n",
              "       58, 17, 20, 56, 56, 14, 24, 60, 54, 13, 64, 56, 49,  6, 12, 21, 35,\n",
              "       45, 50, 42, 20, 27, 44, 20, 55, 50, 27, 31, 36, 63,  4, 35, 51,  0,\n",
              "       42, 53, 57, 28, 22,  8, 20, 48, 31,  8, 60,  3, 20, 19, 48, 28, 50,\n",
              "       25, 39, 48, 21, 20, 41, 23, 34, 30, 10, 57, 32,  8, 44, 53, 31, 46,\n",
              "       54, 56, 27, 56,  4, 20, 45, 43, 55, 35, 51, 13, 27, 57, 55, 16, 31,\n",
              "       15, 28, 37, 52, 28, 28, 45, 31, 47, 44, 40, 31, 22,  0, 14,  3, 62,\n",
              "       12,  0, 19, 63, 47, 15, 15, 13, 31, 54, 42, 61,  3,  6, 50, 53,  3,\n",
              "       30, 58, 29, 64, 19, 45, 64, 15, 44, 31, 62, 52, 44, 22, 58, 19, 33,\n",
              "       53, 52, 39, 19, 48, 50, 29, 32, 60,  5, 25,  7, 39, 20, 15,  9,  9,\n",
              "       11, 50, 57,  1, 36, 30, 40, 16,  5, 62, 41,  6,  6, 50, 26, 30,  0,\n",
              "       26,  4, 32, 34, 29, 49, 37, 24, 25, 45, 48, 31,  7, 28, 47, 45, 58,\n",
              "       42,  9, 27, 33, 23, 54, 14, 58, 15, 60, 26, 26, 44, 27, 11, 47, 47,\n",
              "       16, 14,  7, 34, 18, 20, 17, 21, 11,  4, 23, 28, 45,  7, 52, 51, 44,\n",
              "       62, 42, 14, 38, 50, 15, 47,  4,  9, 27, 31, 18, 30, 16, 49, 22, 28,\n",
              "       47, 54, 33, 64, 53, 21, 12, 16, 38, 53, 21, 32, 25,  1,  6, 54,  8,\n",
              "       48, 29, 17, 27,  3, 46, 29, 31, 36, 35, 25, 17, 34, 20,  0, 14, 51,\n",
              "        6,  0, 37,  1, 12,  0,  9, 28, 51, 58,  4, 28, 55, 49, 62, 14, 33,\n",
              "       35, 64, 18, 47, 59, 28, 49, 38, 31, 57, 28, 53,  5, 34, 15, 15, 21,\n",
              "       21, 49, 46, 23, 18,  2, 58,  9, 48, 57, 53, 28, 51, 29, 56, 50, 55,\n",
              "       33, 61, 56, 35, 26, 38, 58, 52, 16,  2,  2, 24, 23, 31, 24,  0, 34,\n",
              "       43, 19, 39, 52, 55, 41, 31, 16, 41, 19,  9, 42, 24, 19, 33, 52, 47,\n",
              "       22,  2, 48, 24, 53, 59, 25, 30, 56, 24, 21, 22, 46, 22, 43, 48, 27,\n",
              "       15, 26, 56, 37, 37, 39, 58, 47, 13, 31, 59, 64,  6, 58,  3, 43,  2,\n",
              "       42, 44, 14, 52, 24, 27, 35,  8, 22, 14, 52, 56,  6, 33, 51, 47, 61,\n",
              "        1, 44, 31, 24, 21, 47, 62, 38, 10, 28, 42, 52, 52, 60, 45, 29, 20,\n",
              "       14, 51, 15, 48, 62, 45, 35, 46, 49, 56, 36, 64, 57, 16, 16, 47, 10,\n",
              "        1, 32, 55, 43, 58, 46, 21, 11, 63, 41, 43, 50, 39, 42, 62, 23,  5,\n",
              "       52, 10, 17,  5, 28, 42, 43, 51, 51, 28, 20,  4, 27, 25, 23, 36,  1,\n",
              "       28, 11, 61, 64, 33, 33,  4, 62, 55, 11, 17, 50, 38, 49])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1LHaeeJdLSg",
        "colab_type": "code",
        "outputId": "e16abad8-71ae-4b59-b5b0-416453d7bdb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]]))) # The regular input batch \n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ]))) # The predictions made by the model W/O training "
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " \"To blazon it, then sweeten with thy breath\\nThis neighbour air, and let rich music's tongue\\nUnfold the imagined happiness that both\\nReceive in either by this dear encounter.\\n\\nJULIET:\\nConceit, more rich in matter than in words,\\nBrags of his substance, not of ornament:\\nThey are but beggars that can count their worth;\\nBut my true love is grown to such excess\\nI cannot sum up sum of half my wealth.\\n\\nFRIAR LAURENCE:\\nCome, come with me, and we will make short work;\\nFor, by your leaves, you shall not stay alone\\nTill holy church incorporate two in one.\\n\\nBENVOLIO:\\nI pray thee, good Mercutio, let's retire:\\nThe day is hot, the Capulets abroad,\\nAnd, if we meet, we shall not scape a brawl;\\nFor now, these hot days, is the mad blood stirring.\\n\\nMERCUTIO:\\nThou art like one of those fellows that when he\\nenters the confines of a tavern claps me his sword\\nupon the table and says 'God send me no need of\\nthee!' and by the operation of the second cup draws\\nit on the drawer, when indeed there is no need.\\n\\nBENVO\"\n",
            "\n",
            "Next Char Predictions: \n",
            " \"Jv\\nkpljwKkMA3ezINKhmIdHVq'liYz-kDLzfx.si$BpJwJciWiWT3.g$GGMMeKQufb,FFJ:ta'S:Ticspwn&cs.Ex3Z,mb&p lBd&ddzGT,pWsgK3DWIL-ezeVTI!xRkkE?- ;YUswRCXYdYRx\\n,NxZd?,?C!VQZcF\\nxU$Sr;.amPadTr$zkFK\\nJ,;RclSRI$F'X;xaQ'Z:! kI? -sn3w&f!erSpDr,FjiBdxgsSjkX,ljhYIXAu'N-3AslnopRx,al?nwTNvDMxXQ'&vjfbc!jDoHiH\\ngSV!blJMzt'.vGvbvQvMcz!RTCiWkpJcQ?-psSNJAngcy-hDU;r'rgVNt-G..KEy,sqG;y'fKges$TfRkUYIKi$:J!MF'gayEB!GisIbGdiV$x Iz\\nty.M3n,jlcmgxVjj&3ZL\\n$P3m$Nx,kiGPpJ-EG'brIT?W;QX!WpDr\\ntS'j.SsKE\\n&oA,juR\\n$:v\\nPqc\\nTQkBWHpjgVZ Xzyj$FauJwHIvfpiUptEHrrBLvpAzrk,?IWgldHOfHqlOSXy&Wm\\ndosPJ.HjS.v$HGjPlMajIHcKVR:sT.foShprOr&HgeqWmAOsqDSCPYnPPgSifbSJ\\nB$x?\\nGyiCCASpdw$,lo$RtQzGgzCfSxnfJtGUonaGjlQTv'M-aHC33;ls XRbD'xc,,lNR\\nN&TVQkYLMgjS-Pigtd3OUKpBtCvNNfO;iiDB-VFHEI;&KPg-nmfxdBZlCi&3OSFRDkJPipUzoI?DZoITM ,p.jQEO$hQSXWMEVH\\nBm,\\nY ?\\n3Pmt&PqkxBUWzFiuPkZSsPo'VCCIIkhKF!t3jsoPmQrlqUwrWNZtnD!!LKSL\\nVeGanqcSDcG3dLGUniJ!jLouMRrLIJhJejOCNrYYatiASuz,t$e!dfBnLOW.JBnr,Umiw fSLIixZ:PdnnvgQHBmCjxgWhkrXzsDDi: TqethI;yceladxK'n:E'PdemmPH&OMKX P;wzUU&xq;ElZk\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-3vMukYdLSl",
        "colab_type": "text"
      },
      "source": [
        "## Train the Model\n",
        "https://www.tensorflow.org/tutorials/text/text_generation#train_the_model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyHk07QBdLSl",
        "colab_type": "code",
        "outputId": "9aaf6868-304c-437a-e28c-7764b9615bc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def loss(labels, logits): #takes in labels and logits. Logits are raw predictions that have not gone through the normalization process by the model.\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True) # calculates sparse categorial crossentropy loss, from_logits=True when the model is returning logits\n",
        "\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions) \n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \"# (base_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:       \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 1000, 65) # (base_size, sequence_length, vocab_size)\n",
            "scalar_loss:        4.174617\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxRl-79qdLSs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss) #specific optomizer which is called adam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vntS-IqdLSz",
        "colab_type": "text"
      },
      "source": [
        "## Configure Checkpoints\n",
        "https://www.tensorflow.org/tutorials/text/text_generation#configure_checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Gg9bC9FdLS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoints files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\") #checkpoints are used to save specific points during the process of the model\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint( # Save checkpoints at a specific frequency\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8p-OJygRdLS5",
        "colab_type": "text"
      },
      "source": [
        "## Execute the Training\n",
        "To keep training time reasonable, use 10 epochs to train the model. In Colab, set the runtime to GPU for faster training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6xAzLh9dLS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# unit of time, increasing the epochs would decrease the loss function, thus training the model harder for better results. Pushing this up would help but runs longer and harder. \n",
        "EPOCHS=10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lebl7AWMdLS9",
        "colab_type": "code",
        "outputId": "7a2d677a-51b4-400a-aecc-20470a3ffcee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback]) #fitting into the model the dataset, the epochs designated before (10), and the checkpoint callbacks previously mentioned."
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "17/17 [==============================] - 19s 1s/step - loss: 4.2005\n",
            "Epoch 2/10\n",
            "17/17 [==============================] - 19s 1s/step - loss: 3.1724\n",
            "Epoch 3/10\n",
            "17/17 [==============================] - 19s 1s/step - loss: 2.8046\n",
            "Epoch 4/10\n",
            "17/17 [==============================] - 19s 1s/step - loss: 2.5571\n",
            "Epoch 5/10\n",
            "17/17 [==============================] - 19s 1s/step - loss: 2.4113\n",
            "Epoch 6/10\n",
            "17/17 [==============================] - 19s 1s/step - loss: 2.3275\n",
            "Epoch 7/10\n",
            "17/17 [==============================] - 19s 1s/step - loss: 2.2619\n",
            "Epoch 8/10\n",
            "17/17 [==============================] - 19s 1s/step - loss: 2.2021\n",
            "Epoch 9/10\n",
            "17/17 [==============================] - 19s 1s/step - loss: 2.1409\n",
            "Epoch 10/10\n",
            "17/17 [==============================] - 19s 1s/step - loss: 2.0776\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c44JvG8ifhH_",
        "colab_type": "text"
      },
      "source": [
        "## Generate Text\n",
        "### Restore the Latest Checkpooint\n",
        "https://www.tensorflow.org/tutorials/text/text_generation#generate_text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kHYG2YV5cMn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2c55c710-589b-4546-9069-df57232c109d"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir) # Going back to a specific checkpoint since the model can fit a specific batch size only"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_10'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "barcHB97gFRZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units,batch_size=1) # to make the model with a different batch size, you would have to rebuild and restore weights from the checkpoint\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrjc3DmwgnUL",
        "colab_type": "code",
        "outputId": "f0d5c3e1-037b-48ac-c1af-32d00c00a7f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            16640     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 65)             66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGhagIIEjlYB",
        "colab_type": "text"
      },
      "source": [
        "## The Prediction Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcHDgtAMjtSB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "  \n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start to string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures result in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    # remove the batch dimension\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    # using a categorical distribution to predict the character returned by the model\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    # We pass the predicted character as the next input to the model\n",
        "    # along with previous hidden state\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "  \n",
        "  return (start_string +''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u574owPClWCB",
        "colab_type": "code",
        "outputId": "9b80bd6e-7678-4d38-c261-c0ac0d2acee5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "print(generate_text(model, start_string=u'ROMEO: ')) #small number of epochs leads the model to generate predictions that arent coherent sentences."
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO: for:\n",
            "Thells sis bute hang. Nust bety,\n",
            "Soud our the saith claryound,\n",
            "Bul-chat nole, leat to nor wtill hen my tofer if that your.\n",
            "\n",
            "ETREBYI:\n",
            "I whan hay fous fare, in I he. mus is nts se, theru.\n",
            "\n",
            "LUCHIRD:\n",
            "Me pay all di'g is wo mack.\n",
            "\n",
            "USANS:\n",
            "My, miseno'd for sith!\n",
            "BEs Ior of trot ioth fit.\n",
            "\n",
            "SINCHOR:\n",
            "Ap thim, will is pade ale tienee merswions fonarin d;\n",
            "char mo thece nows leate ou pror o frisp, than dow.\n",
            "Hand fathy get keno,\n",
            "Baded itiunt on 'd tofm wood kine hearSs comsembends;\n",
            "His lood, foul ar Wart, I love, in hen woors arres thou heme\n",
            "Toccuss, whares if duknd pey mapt tour.\n",
            "\n",
            "ELHBUN:\n",
            "If lord mirss, gally, with letide tillvild wnou 'park.\n",
            "\n",
            "FOrCYORRZRY:\n",
            "To, wir, then hesee ranty, sithter.\n",
            "\n",
            "USFORA:\n",
            "Ascomly, shepter pay netinges wrar wh tich;\n",
            "Ond thes upatby, knaw, wead is dpies soy.\n",
            "\n",
            "PAMEIT:\n",
            "Aly lin fagringarn dook wxact dietet's wing;\n",
            "Andukins is mis in beerid dist preawe you.\n",
            "\n",
            "SAglingt wim,\n",
            "Fat I all thee thee tan thy wich, That lisby;\n",
            "Aes and fom of on and he to go tods ofe, hioks fougt\n",
            "No\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-c9XBn9hnDPf",
        "colab_type": "text"
      },
      "source": [
        "The easiest thing you can do to improve the results it to train it for longer (try EPOCHS=30).\n",
        "\n",
        "You can also experiment with a different start string, or try adding another RNN layer to improve the model's accuracy, or adjusting the temperature parameter to generate more or less random predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj2c5V3DqC4U",
        "colab_type": "text"
      },
      "source": [
        "## Advanced: Customized Training \n",
        "\n",
        "The above training procedure is simple, but does not give you much control.\n",
        "\n",
        "So now that you've seen how to run the model manually let's unpack the training loop, and implement it ourselves. This gives a starting point if, for example, to implement curriculum learning to help stabilize the model's open-loop output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxAcPbLOqkAy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "    vocab_size = len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WG8Vx11q6Wp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcy_Oge5q9nL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, target):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(inp)\n",
        "    loss = tf.reduce_mean(\n",
        "        tf.keras.losses.sparse_categorical_crossentropy(\n",
        "            target, predictions, from_logits=True))\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTO7f9zrrZlU",
        "colab_type": "code",
        "outputId": "245b6375-bbea-47c7-e046-7018d9eb5936",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "# Training step\n",
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  # initializing the hidden state at the start of every epoch\n",
        "  # initally hidden is None\n",
        "  hidden = model.reset_states()\n",
        "\n",
        "  for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "    loss = train_step(inp, target)\n",
        "\n",
        "    if batch_n % 100 == 0:\n",
        "      template = 'Epoch {} Batch {} Loss {}'\n",
        "      print(template.format(epoch+1, batch_n, loss))\n",
        "\n",
        "  # saving (checkpoint) the model every 5 epochs\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "  print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
        "  print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 4.175279140472412\n",
            "Epoch 1 Batch 100 Loss 2.3285584449768066\n",
            "Epoch 1 Loss 2.1420\n",
            "Time taken for 1 epoch 8.299541711807251 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 2.10976505279541\n",
            "Epoch 2 Batch 100 Loss 1.9365309476852417\n",
            "Epoch 2 Loss 1.7947\n",
            "Time taken for 1 epoch 7.230211496353149 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.7390222549438477\n",
            "Epoch 3 Batch 100 Loss 1.6487317085266113\n",
            "Epoch 3 Loss 1.5598\n",
            "Time taken for 1 epoch 7.26666784286499 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.5677311420440674\n",
            "Epoch 4 Batch 100 Loss 1.5128766298294067\n",
            "Epoch 4 Loss 1.4948\n",
            "Time taken for 1 epoch 7.295722961425781 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.481947422027588\n",
            "Epoch 5 Batch 100 Loss 1.4040343761444092\n",
            "Epoch 5 Loss 1.4222\n",
            "Time taken for 1 epoch 7.355914354324341 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.3769409656524658\n",
            "Epoch 6 Batch 100 Loss 1.4182506799697876\n",
            "Epoch 6 Loss 1.4112\n",
            "Time taken for 1 epoch 7.293524980545044 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.3176634311676025\n",
            "Epoch 7 Batch 100 Loss 1.357419729232788\n",
            "Epoch 7 Loss 1.3546\n",
            "Time taken for 1 epoch 7.266653776168823 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.268174409866333\n",
            "Epoch 8 Batch 100 Loss 1.3212449550628662\n",
            "Epoch 8 Loss 1.2950\n",
            "Time taken for 1 epoch 7.28155255317688 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.296570897102356\n",
            "Epoch 9 Batch 100 Loss 1.2757083177566528\n",
            "Epoch 9 Loss 1.2729\n",
            "Time taken for 1 epoch 7.242734909057617 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.2034167051315308\n",
            "Epoch 10 Batch 100 Loss 1.2429617643356323\n",
            "Epoch 10 Loss 1.2608\n",
            "Time taken for 1 epoch 7.3065807819366455 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekSCIldxuRHa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}