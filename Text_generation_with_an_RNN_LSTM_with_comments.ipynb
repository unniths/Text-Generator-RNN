{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Text generation with an RNN/LSTM with comments.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unniths/Text-Generator-RNN/blob/master/Text_generation_with_an_RNN_LSTM_with_comments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-NJ8L8mdLQ1",
        "colab_type": "text"
      },
      "source": [
        "## Shiva Unnithan\n",
        "# Text generation with an RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEWbvfc3dLQ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48BsfnnIdLQ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('wonderland.txt', 'https://raw.githubusercontent.com/unniths/Text-Generator-RNN/master/wonderland.txt') # instead of using the shakespeare source, I'm using a cleaned up version of \n",
        "                                                                                                                                               # Alice in Wonderland that had everything but its text removed. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bK3tF6LAdLRC",
        "colab_type": "code",
        "outputId": "9cad1dab-8ee7-43d6-bcd5-becc0a26e8a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Read, then decode for py2 compat\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} character'.format(len(text)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 143552 character\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTWByUlldLRI",
        "colab_type": "code",
        "outputId": "bdf50b1d-e253-4814-b607-337792278068",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "print(text[:1000]) # printing the first 1000 characters"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Alice was beginning to get very tired of sitting by her sister on the\n",
            "bank, and of having nothing to do: once or twice she had peeped into the\n",
            "book her sister was reading, but it had no pictures or conversations in\n",
            "it, 'and what is the use of a book,' thought Alice 'without pictures or\n",
            "conversations?'\n",
            "\n",
            "So she was considering in her own mind (as well as she could, for the\n",
            "hot day made her feel very sleepy and stupid), whether the pleasure\n",
            "of making a daisy-chain would be worth the trouble of getting up and\n",
            "picking the daisies, when suddenly a White Rabbit with pink eyes ran\n",
            "close by her.\n",
            "\n",
            "There was nothing so VERY remarkable in that; nor did Alice think it so\n",
            "VERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\n",
            "Oh dear! I shall be late!' (when she thought it over afterwards, it\n",
            "occurred to her that she ought to have wondered at this, but at the time\n",
            "it all seemed quite natural); but when the Rabbit actually TOOK A WATCH\n",
            "OUT OF ITS WAISTCOAT-POCKET, and looked at it, and \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lfYxRN4dLRN",
        "colab_type": "code",
        "outputId": "11383ca9-380d-407a-8225-37543d7b6162",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))\n",
        "#print(len(vocab))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "67 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5HbURAIdLRS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a mapping from unique characters to indices \n",
        "char2idx = {u:i for i, u in enumerate(vocab)} # look up table to map characters to numbers\n",
        "idx2char = np.array(vocab) \n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gh42c8WdLRW",
        "colab_type": "code",
        "outputId": "3f68693f-b8c0-4235-89b5-313c11167899",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print('{')\n",
        "for char,_ in zip(char2idx, range(67)): # printing the range of all unique characters just to visualize what each character is represented with\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char])) #formatting to see which character represents which integer\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  '!' :   2,\n",
            "  '\"' :   3,\n",
            "  \"'\" :   4,\n",
            "  '(' :   5,\n",
            "  ')' :   6,\n",
            "  ',' :   7,\n",
            "  '-' :   8,\n",
            "  '.' :   9,\n",
            "  ':' :  10,\n",
            "  ';' :  11,\n",
            "  '?' :  12,\n",
            "  'A' :  13,\n",
            "  'B' :  14,\n",
            "  'C' :  15,\n",
            "  'D' :  16,\n",
            "  'E' :  17,\n",
            "  'F' :  18,\n",
            "  'G' :  19,\n",
            "  'H' :  20,\n",
            "  'I' :  21,\n",
            "  'J' :  22,\n",
            "  'K' :  23,\n",
            "  'L' :  24,\n",
            "  'M' :  25,\n",
            "  'N' :  26,\n",
            "  'O' :  27,\n",
            "  'P' :  28,\n",
            "  'Q' :  29,\n",
            "  'R' :  30,\n",
            "  'S' :  31,\n",
            "  'T' :  32,\n",
            "  'U' :  33,\n",
            "  'V' :  34,\n",
            "  'W' :  35,\n",
            "  'Y' :  36,\n",
            "  'Z' :  37,\n",
            "  '[' :  38,\n",
            "  ']' :  39,\n",
            "  '_' :  40,\n",
            "  'a' :  41,\n",
            "  'b' :  42,\n",
            "  'c' :  43,\n",
            "  'd' :  44,\n",
            "  'e' :  45,\n",
            "  'f' :  46,\n",
            "  'g' :  47,\n",
            "  'h' :  48,\n",
            "  'i' :  49,\n",
            "  'j' :  50,\n",
            "  'k' :  51,\n",
            "  'l' :  52,\n",
            "  'm' :  53,\n",
            "  'n' :  54,\n",
            "  'o' :  55,\n",
            "  'p' :  56,\n",
            "  'q' :  57,\n",
            "  'r' :  58,\n",
            "  's' :  59,\n",
            "  't' :  60,\n",
            "  'u' :  61,\n",
            "  'v' :  62,\n",
            "  'w' :  63,\n",
            "  'x' :  64,\n",
            "  'y' :  65,\n",
            "  'z' :  66,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HNui02GdLRb",
        "colab_type": "code",
        "outputId": "3bb52e69-0f77-4762-9edc-842a7069403e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('{} ---- characters mapped to int ----> {}'.format(repr(text[:13]), text_as_int[:13])) # Quick example of using the first 13 characters turned into 13 corresponding integers"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Alice was beg' ---- characters mapped to int ----> [13 52 49 43 45  1 63 41 59  1 42 45 47]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MSgMc26dLRg",
        "colab_type": "code",
        "outputId": "306328e6-9ad8-4972-cdc6-559747db9b2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100 # length of text measured by characters\n",
        "# examples_per_epoch = len(text)//(seq_length+1) \n",
        "# dataX = []\n",
        "# dataY = []\n",
        "# Create training examples / target\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int) # Converting the text vector into a stream of character indices\n",
        "\n",
        "for i in char_dataset.take(5): #take the first 5 characters from the dataset\n",
        "    print(idx2char[i.numpy()]) #print the first five indices from the array using idx2char"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A\n",
            "l\n",
            "i\n",
            "c\n",
            "e\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mM1eWkbcdLRk",
        "colab_type": "code",
        "outputId": "a3954bba-cf41-4317-f1e9-ec498c9dab45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True) # in dataset, batch METHOD converts individual characters to sequences or chunks\n",
        "\n",
        "for item in sequences.take(5): # 5 is the desired length for the sequence\n",
        "    print(repr(''.join(idx2char[item.numpy()]))) "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Alice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to '\n",
            "'do: once or twice she had peeped into the\\nbook her sister was reading, but it had no pictures or conv'\n",
            "\"ersations in\\nit, 'and what is the use of a book,' thought Alice 'without pictures or\\nconversations?'\\n\"\n",
            "'\\nSo she was considering in her own mind (as well as she could, for the\\nhot day made her feel very sle'\n",
            "'epy and stupid), whether the pleasure\\nof making a daisy-chain would be worth the trouble of getting u'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4Y6T2F_dLRp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk): \n",
        "    input_text = chunk[:-1] # seq_length -> input\n",
        "    target_text = chunk[1:] # seq_length+1 -> target\n",
        "    return input_text, target_text # the values of -1 and 1 have to do with the tanh squashing function which is used in RNN.\n",
        "\n",
        "dataset = sequences.map(split_input_target) # using map function to apply this method to each batch. We are duplicating and shifting once to the right to create input/target for each batch. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvVV0SUUdLRx",
        "colab_type": "code",
        "outputId": "be1ebb66-e722-4db8-a34c-a4ed4e54cba8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# This prints the example input value and target value, which also shows the way the data works. Input has the A in the beginning of the sentence but no \"?\", while target has no A but does have the \"?\" since it is shifted once.\n",
        "for input_example, target_example in dataset.take(1): \n",
        "    print('Input data: ', repr(''. join(idx2char[input_example.numpy()])))\n",
        "    print('# of Characters: {} '.format(len(idx2char[input_example.numpy()])))\n",
        "    print('Target data: ', repr(''.join(idx2char[target_example.numpy()]))) \n",
        "    print('# of Characters: {} '.format(len(idx2char[target_example.numpy()])))\n",
        "# added the extra print statements just to show the same amount of characters are being used that was defined before in seq_length"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'Alice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to'\n",
            "# of Characters: 100 \n",
            "Target data:  'lice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to '\n",
            "# of Characters: 100 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nnv31rcvdLR1",
        "colab_type": "code",
        "outputId": "c0923683-c012-4a48-fbbb-0551b716c9ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# This part is showing how the model is predicting the next character. So the model first gets index 13 (A), and expects the index 52 (l) afterwards. \n",
        "# When it puts the input of l, RNN means it would remember the previous results and continue with predictions.\n",
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "  input: 13 ('A')\n",
            "  expected output: 52 ('l')\n",
            "Step    1\n",
            "  input: 52 ('l')\n",
            "  expected output: 49 ('i')\n",
            "Step    2\n",
            "  input: 49 ('i')\n",
            "  expected output: 43 ('c')\n",
            "Step    3\n",
            "  input: 43 ('c')\n",
            "  expected output: 45 ('e')\n",
            "Step    4\n",
            "  input: 45 ('e')\n",
            "  expected output: 1 (' ')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roVVb_c7dLR4",
        "colab_type": "text"
      },
      "source": [
        "## Create Training Batches\n",
        "https://www.tensorflow.org/tutorials/text/text_generation#create_training_batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkfAGdxudLR5",
        "colab_type": "code",
        "outputId": "697da8ef-e215-4934-93ff-a4b84ce10ce9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Batch size means how big a batch of data that was split from the text source will be\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset \n",
        "# (TF data is design to work with possibly infinite sequences, so it doesn't attempt to shuffle the entire sequence in memory.\n",
        "# Instead, it maintains a buffer in which is shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MI5wC8KxdLR_",
        "colab_type": "text"
      },
      "source": [
        "## Build The Model\n",
        "https://www.tensorflow.org/tutorials/text/text_generation#build_the_model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j11YxvU7dLSA",
        "colab_type": "code",
        "outputId": "f44d370b-02be-4e1c-b1e1-fa530e216852",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension \n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units \n",
        "rnn_units = 1024\n",
        "print(vocab_size)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "67\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "au-4X8KUdLSF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([ # Sequential is used to group a linear stack of layers. Used since all the layers have a single input and produce a single output. \n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]), # Embedding is the input layer. A trainable lookup table that will map numbers of each char to a vector using embedding_dim dimensions.\n",
        "        tf.keras.layers.LSTM(rnn_units, # SWITCHING TO LSTM LAYER INSTEAD, GRU IS A TYPE OF RNN WITH A SIZE TO MAKE IT MORE ACCURATE\n",
        "                           return_sequences=True,\n",
        "                           stateful=True,\n",
        "                           recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dense(vocab_size) # Output layer, output are determined by vocab_size\n",
        "    ])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YB1UGKXI-XpI",
        "colab_type": "text"
      },
      "source": [
        "## **Why I switched from GRU to LSTM**\n",
        "While GRU has two gates (the reset and update gates), LSTM uses three gates (input, output, and forget). In this case, LSTMs are better for longer training data but do train slower. This results in where LSTMs remember longer sequences thus having more accurate predictions in the longer run. Both of these are used in order to fix the issue that RNN models have which is the Short Term memory that the model presents. Without either of these units, the model would be producing sentences that do not make sense in the context of the language it is being written in.\n",
        "\n",
        "An example that I would use to explain this using the sentence \"Shiva saw Zhao.\" Without LSTM or GRU, the model would make sentences that make sense gramatically but don't make sense in the context of the text such as: \"Shiva saw Doug.\" or \"Zhao saw.\" or even \"Zhao.\". \n",
        "\n",
        "What LSTM does can be explained better in the [YouTube tutorial by Brandon Rohrer](https://www.youtube.com/watch?v=WCUNPb-5EYI) that I used in order to better understand LSTM, but the basics is that for every word (or character in our model's case), it passes through a memory of every word it has taken before to see if the word that is going to be outputted makes sense. Every possible word choice is being shaved down more and more until it finds an output that makes sense as a prediction. However, a part of that prediction is then inputted next time with the NEXT input and goes through each gate as a memory to make sure we don't get repeated words nor a word that doesn't make sense in the context of what we've had before. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Snez7aifdLSJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "    vocab_size = len(vocab), # 65\n",
        "    embedding_dim=embedding_dim, #using embedding dm which we declared as 256\n",
        "    rnn_units=rnn_units, # using rnn_unnits which was declared at 1024 \n",
        "    batch_size=BATCH_SIZE) # using BATCH_SIZE which was declared as 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLwsr2SZdLSM",
        "colab_type": "text"
      },
      "source": [
        "## Try the Model\n",
        "https://www.tensorflow.org/tutorials/text/text_generation#try_the_model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vc1I3qVXdLSN",
        "colab_type": "code",
        "outputId": "8cf0ea52-18e3-4acf-c904-9721bef3fb5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Testing the model\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 67) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZkbuU3PdLSR",
        "colab_type": "code",
        "outputId": "70788c94-c7b6-45dd-ff50-e9382948292e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model.summary() # summary to show every individual layer of the model"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           17152     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 67)            68675     \n",
            "=================================================================\n",
            "Total params: 5,332,803\n",
            "Trainable params: 5,332,803\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYZ_LZ19dLSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1) # First examle in the batch\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy() # removing a specific axis of size (-1) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKMptTa6dLSc",
        "colab_type": "code",
        "outputId": "9d78d0fe-e658-42dd-a243-6196c705579c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([32, 60,  7, 31, 55, 30, 11, 14, 49, 30, 51, 47, 39, 10, 57, 55, 18,\n",
              "       44,  0, 59, 58, 24, 59, 20, 30, 55,  5, 38,  6, 35, 13, 39, 50, 32,\n",
              "       14, 32, 11,  2, 43, 26,  8,  9,  0, 51, 22, 49,  9, 10, 27, 59, 22,\n",
              "       18,  5, 46, 28,  5, 60, 59, 10,  4, 27, 25, 59, 12,  7, 57, 52, 15,\n",
              "       54, 16, 40, 24, 29,  1, 54, 18, 19, 18, 19, 43, 62, 43, 48, 10, 10,\n",
              "       16, 48, 16, 38, 40,  1, 59,  0, 13, 37, 25, 48, 62, 33, 17])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1LHaeeJdLSg",
        "colab_type": "code",
        "outputId": "5e345d5d-84a3-403a-bb37-950b1bf4a0ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]]))) # The regular input batch \n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ]))) # The predictions made by the model W/O training "
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " \"They can't have anything to put\\ndown yet, before the trial's begun.'\\n\\n'They're putting down their na\"\n",
            "\n",
            "Next Char Predictions: \n",
            " \"Tt,SoR;BiRkg]:qoFd\\nsrLsHRo([)WA]jTBT;!cN-.\\nkJi.:OsJF(fP(ts:'OMs?,qlCnD_LQ nFGFGcvch::DhD[_ s\\nAZMhvUE\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vntS-IqdLSz",
        "colab_type": "text"
      },
      "source": [
        "## Configure Checkpoints\n",
        "https://www.tensorflow.org/tutorials/text/text_generation#configure_checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Gg9bC9FdLS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoints files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\") #checkpoints are used to save specific points during the process of the model\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint( # Save checkpoints at a specific frequency\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-3vMukYdLSl",
        "colab_type": "text"
      },
      "source": [
        "## Train the Model\n",
        "https://www.tensorflow.org/tutorials/text/text_generation#train_the_model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyHk07QBdLSl",
        "colab_type": "code",
        "outputId": "f750550a-2764-4b0f-933a-854b67fd0922",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def loss(labels, logits): #takes in labels and logits. Logits are raw predictions that have not gone through the normalization process by the model.\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True) # calculates sparse categorial crossentropy loss, from_logits=True when the model is returning logits\n",
        "\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions) \n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \"# (base_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:       \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 67) # (base_size, sequence_length, vocab_size)\n",
            "scalar_loss:        4.2050767\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxRl-79qdLSs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss) #specific optomizer which is called adam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8p-OJygRdLS5",
        "colab_type": "text"
      },
      "source": [
        "## Execute the Training\n",
        "To keep training time reasonable, use 10 epochs to train the model. In Colab, set the runtime to GPU for faster training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6xAzLh9dLS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# unit of time, increasing the epochs would decrease the loss function, thus training the model harder for better results. Pushing this up would help but runs longer and harder. \n",
        "EPOCHS=50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lebl7AWMdLS9",
        "colab_type": "code",
        "outputId": "96120cba-d036-40cb-fa20-9ab16b1c75a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback]) #fitting into the model the dataset, the epochs designated before (20), and the checkpoint callbacks previously mentioned."
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 3.3002\n",
            "Epoch 2/50\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 2.9968\n",
            "Epoch 3/50\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 2.6321\n",
            "Epoch 4/50\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 2.4040\n",
            "Epoch 5/50\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 2.2444\n",
            "Epoch 6/50\n",
            "22/22 [==============================] - 2s 85ms/step - loss: 2.1095\n",
            "Epoch 7/50\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 1.9877\n",
            "Epoch 8/50\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 1.8829\n",
            "Epoch 9/50\n",
            "22/22 [==============================] - 2s 88ms/step - loss: 1.7935\n",
            "Epoch 10/50\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 1.7048\n",
            "Epoch 11/50\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 1.6296\n",
            "Epoch 12/50\n",
            "22/22 [==============================] - 2s 88ms/step - loss: 1.5584\n",
            "Epoch 13/50\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 1.4934\n",
            "Epoch 14/50\n",
            "22/22 [==============================] - 2s 88ms/step - loss: 1.4315\n",
            "Epoch 15/50\n",
            "22/22 [==============================] - 2s 88ms/step - loss: 1.3696\n",
            "Epoch 16/50\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 1.3116\n",
            "Epoch 17/50\n",
            "22/22 [==============================] - 2s 88ms/step - loss: 1.2559\n",
            "Epoch 18/50\n",
            "22/22 [==============================] - 2s 88ms/step - loss: 1.1992\n",
            "Epoch 19/50\n",
            "22/22 [==============================] - 2s 88ms/step - loss: 1.1453\n",
            "Epoch 20/50\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 1.0859\n",
            "Epoch 21/50\n",
            "22/22 [==============================] - 2s 88ms/step - loss: 1.0305\n",
            "Epoch 22/50\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.9766\n",
            "Epoch 23/50\n",
            "22/22 [==============================] - 2s 89ms/step - loss: 0.9167\n",
            "Epoch 24/50\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.8552\n",
            "Epoch 25/50\n",
            "22/22 [==============================] - 2s 88ms/step - loss: 0.7954\n",
            "Epoch 26/50\n",
            "22/22 [==============================] - 2s 86ms/step - loss: 0.7383\n",
            "Epoch 27/50\n",
            "22/22 [==============================] - 2s 89ms/step - loss: 0.6841\n",
            "Epoch 28/50\n",
            "22/22 [==============================] - 2s 88ms/step - loss: 0.6287\n",
            "Epoch 29/50\n",
            "22/22 [==============================] - 2s 89ms/step - loss: 0.5787\n",
            "Epoch 30/50\n",
            "22/22 [==============================] - 2s 88ms/step - loss: 0.5346\n",
            "Epoch 31/50\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4919\n",
            "Epoch 32/50\n",
            "22/22 [==============================] - 2s 89ms/step - loss: 0.4537\n",
            "Epoch 33/50\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.4188\n",
            "Epoch 34/50\n",
            "22/22 [==============================] - 2s 88ms/step - loss: 0.3906\n",
            "Epoch 35/50\n",
            "22/22 [==============================] - 2s 88ms/step - loss: 0.3673\n",
            "Epoch 36/50\n",
            "22/22 [==============================] - 2s 89ms/step - loss: 0.3471\n",
            "Epoch 37/50\n",
            "22/22 [==============================] - 2s 88ms/step - loss: 0.3285\n",
            "Epoch 38/50\n",
            "22/22 [==============================] - 2s 89ms/step - loss: 0.3117\n",
            "Epoch 39/50\n",
            "22/22 [==============================] - 2s 89ms/step - loss: 0.2971\n",
            "Epoch 40/50\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.2871\n",
            "Epoch 41/50\n",
            "22/22 [==============================] - 2s 89ms/step - loss: 0.2767\n",
            "Epoch 42/50\n",
            "22/22 [==============================] - 2s 88ms/step - loss: 0.2681\n",
            "Epoch 43/50\n",
            "22/22 [==============================] - 2s 89ms/step - loss: 0.2611\n",
            "Epoch 44/50\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.2540\n",
            "Epoch 45/50\n",
            "22/22 [==============================] - 2s 91ms/step - loss: 0.2466\n",
            "Epoch 46/50\n",
            "22/22 [==============================] - 2s 90ms/step - loss: 0.2390\n",
            "Epoch 47/50\n",
            "22/22 [==============================] - 2s 89ms/step - loss: 0.2345\n",
            "Epoch 48/50\n",
            "22/22 [==============================] - 2s 92ms/step - loss: 0.2329\n",
            "Epoch 49/50\n",
            "22/22 [==============================] - 2s 87ms/step - loss: 0.2245\n",
            "Epoch 50/50\n",
            "22/22 [==============================] - 2s 90ms/step - loss: 0.2211\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c44JvG8ifhH_",
        "colab_type": "text"
      },
      "source": [
        "## Generate Text\n",
        "### Restore the Latest Checkpooint\n",
        "https://www.tensorflow.org/tutorials/text/text_generation#generate_text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kHYG2YV5cMn",
        "colab_type": "code",
        "outputId": "d027868b-9348-4362-a878-db68be4c4d2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir) # Going back to a latest checkpoint since the model can fit a specific batch size only"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_50'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "barcHB97gFRZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units,batch_size=1) # to make the model with a different batch size, you would have to rebuild and restore weights from the checkpoint\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir)) #loading the last checkpoint/weight which has the lowest lowest\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrjc3DmwgnUL",
        "colab_type": "code",
        "outputId": "7822e519-4d96-433a-f8ab-1140809357b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            17152     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (1, None, 1024)           5246976   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 67)             68675     \n",
            "=================================================================\n",
            "Total params: 5,332,803\n",
            "Trainable params: 5,332,803\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGhagIIEjlYB",
        "colab_type": "text"
      },
      "source": [
        "## The Prediction Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcHDgtAMjtSB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "  \n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start to string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures result in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 0.6\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    # remove the batch dimension\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    # using a categorical distribution to predict the character returned by the model\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    # We pass the predicted character as the next input to the model\n",
        "    # along with previous hidden state\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "  \n",
        "  return (start_string +''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u574owPClWCB",
        "colab_type": "code",
        "outputId": "ebd7a5f4-70a6-47a1-a1df-0bbb81c35e8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "print(generate_text(model, start_string=u'Alice')) # Starting string affects the the result that you are going to get.\n",
        "                                                   # If I used the same starting string \"ROMEO: \" as the tutorial, my results would have been wildly different due to the original text"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Alice 'WhAnke the whiting kindly twon the end of the accident, Oh, there were all quarrel so dream,\n",
            "    \"Hat you ters a voice of the end--' the Hatter was the first to break the silence. 'What day of the month\n",
            "is it?' he said, turning to the jury, in a low, trembling voice.\n",
            "\n",
            "'There's more evidence Yourself a little\n",
            "queer in such a nice soft thing to nurse--and she's\n",
            "such a capital one for catching mice--ohe\n",
            "was going to begin at the end of it, and on both sides of it, and\n",
            "begun to replaid that save after the\n",
            "right house before.\n",
            "\n",
            "'Of course,' the Dodo solemnly partoould herself falling down a very deep\n",
            "well.\n",
            "\n",
            "Either the well was very deep, or she fell very slowly,\n",
            "but she heard one of\n",
            "them say, 'Look out now, Five! Don't go splashing paint over me like\n",
            "that!'\n",
            "\n",
            "'I could see if she came, near the end of the bat,\n",
            "     And welk triem, and seemed to quiveralle hed that they could not taste\n",
            "theirs, and the small ones choked and had to be patted on the back.\n",
            "How suppose it the made of this replied.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-c9XBn9hnDPf",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "All in all, the most important changes that I have done are the following in this code:\n",
        "\n",
        "1) Changing the source from Shakespeare to Alice in Wonderland like I have seen in many different examples online.\n",
        "  - This was mostly due to the formatting of Shakespearean writing and the play format that it is written in. I think with a bit more training I could have overcome that, but I only realized this after working with the new source and playing around with the settings in the model. \n",
        "\n",
        "2) Changing the type of neural network the RNN is from GRU to LSTMs.\n",
        "  - The model is definitely a lot more complicated with it's nuances, but as I explained in [this cell](https://colab.research.google.com/drive/1Qzi8KNLwdecCY299pjvpE-dHigtD7a2Y#scrollTo=YB1UGKXI-XpI), the neural network would definitely benefit from a larger data source/sequence in order to make more accurate predictions.\n",
        "\n",
        "3) EPOCHS\n",
        "  - EPOCHS as explained in the comments are a unit of time that is related to the inception of Python. In this model, we use EPOCHS as a time variable in order to show how many cycles of training the model does. The larger the EPOCH value, the more training the model gets, and the more accurate it gets with its prediction and generally better with how \"clean\" the predictions look. \n",
        "  - I increased the EPOCHS to 50 personally just to see how much more accurate it looked, and the difference is definitely visible in the sentence structures. The most noticeable part is the decreased amount of typos and the generated text looking much more like an excerpt out of a book rather than conjoined sentences. \n",
        "\n",
        "4) Temperature\n",
        "  - Temperature is a hyperparameter of neural networks including GRU and LSTM. Temperature is related to the predictions given by text generator because it either excites or calms the raw predictions. For example, the normal temperature used by the TensorFlow guide used *temperature = 1.0* which is just the *logits/1.0* meaning the raw prediction data (logits) are just pushed out. With a higher value for temperature, we get more diversity but more mistakes, while a lower value for temperature gives us more conservative results that have less mistakes.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh3czOXMh_Ei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}